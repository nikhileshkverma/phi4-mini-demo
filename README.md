
````markdown
# ðŸ”¥ Phi-4 Mini Instruct - Transformers Inference Demo

This repository demonstrates how to set up and run inference using the [`microsoft/Phi-4-mini-instruct`](https://huggingface.co/microsoft/Phi-4-mini-instruct) language model via Hugging Face Transformers in Python.

Ideal for fast prototyping, instruction-based generation, or learning how to work with large language models (LLMs).

---

## ðŸ“ Project Structure

```bash
phi4-mini-instruct-demo/
â”œâ”€â”€ run_phi4.py           # Inference script using Hugging Face
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ .gitignore            # Ignore virtualenv, pycache, etc.
â””â”€â”€ README.md             # Project documentation (this file)
````

---

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Create a Python Virtual Environment

```bash
python3 -m venv phi-env
source phi-env/bin/activate
```

### 2ï¸âƒ£ Install Required Dependencies

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

Or manually:

```bash
pip install torch transformers
```

> âœ… **Python 3.9 or 3.10** is recommended.

---

## ðŸš€ Run Inference

### `run_phi4.py`

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

def main():
    tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-4-mini-instruct")
    model = AutoModelForCausalLM.from_pretrained("microsoft/Phi-4-mini-instruct")
    model.eval()

    prompt = "Describe about youtube channel @djnikmumabi"
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(**inputs, max_new_tokens=100)

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print("Response:", response)

if __name__ == "__main__":
    main()
```

### Run the script

```bash
python run_phi4.py
```

---

## âœ… Sample Output

```
Response: The YouTube channel @djnikmumabi features DJ mixes, remixes, and party anthems. It showcases the latest in EDM, Bollywood beats, and more...
```

---

## ðŸ“˜ Model Details

* **Model**: `microsoft/Phi-4-mini-instruct`
* **Type**: Causal Language Model (Decoder-only)
* **Use Case**: Instruction-following generation
* **Context Limit**: \~4K tokens
* **License**: MIT (Permissive)

---

## ðŸ“š References & Credits

* ðŸ¤– Model: [Phi-4 Mini Instruct](https://huggingface.co/microsoft/Phi-4-mini-instruct) by Microsoft Research
* ðŸ’» Framework: [Hugging Face Transformers](https://github.com/huggingface/transformers)
* ðŸ”¬ Research: Microsoftâ€™s Phi-2/4 Mini Instruct Series
* ðŸ“¦ Packaging: Built with Python, PyTorch

---

## ðŸªª License

This project is open-sourced under the **MIT License**.
Feel free to use, modify, and share.

---

## ðŸ’¼ Author

**Nikhilesh K Verma**
GitHub: [@nikhileshkverma](https://github.com/nikhileshkverma)

---

## âœ¨ Contributions Welcome!

Feel free to fork the repo, suggest improvements, or raise issues.
PRs are appreciated!

---

## ðŸ”— Related Resources

* [Phi-4 on Hugging Face](https://huggingface.co/microsoft/Phi-4-mini-instruct)
* [Understanding Transformers](https://huggingface.co/transformers/)
* [Hugging Face Course](https://huggingface.co/course)

---

